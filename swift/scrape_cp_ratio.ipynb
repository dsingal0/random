{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ba9e5-cc75-4148-900e-0dc7da3c1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec447e4-69c2-4221-bcca-32ca161e8474",
   "metadata": {},
   "source": [
    "## Run the following cell to collect data again(warning: this will overwrite the pickle file)\n",
    "### Make sure to set the start date and pickle file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d9504-e6b3-4fd5-8eda-14c3d4d9fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape data from CBOE\n",
    "#after 10-4-2019 mm-dd-yyyy\n",
    "#url format yyyy-mm-dd\n",
    "base_url = \"https://markets.cboe.com/us/options/market_statistics/daily/?mkt=cone&dt=\"\n",
    "pickle_file = \"scraped_data.pkl\"\n",
    "\n",
    "def get(url):\n",
    "    headers = {}\n",
    "    try:\n",
    "        resp = requests.get(url)\n",
    "        if resp.ok:\n",
    "            return resp.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return None\n",
    "    \n",
    "\"\"\"This part is to scrape from CBOE website and save the file\"\"\"\n",
    "scrapeDict ={}\n",
    "weekdays=[]\n",
    "start_date = datetime.date(2019, 10, 5)\n",
    "end_date = datetime.date.today()\n",
    "delta = datetime.timedelta(days=1)\n",
    "run_date=start_date\n",
    "while run_date < end_date:\n",
    "    if run_date.weekday() not in [5,6]: #ie. Mon-Fri only\n",
    "        weekdays.append(run_date)\n",
    "    run_date += delta\n",
    "print(len(weekdays))\n",
    "\n",
    "print('Running:') \n",
    "# this loop loops around for some reason, not sure why\n",
    "for get_date in weekdays:\n",
    "    html_date = datetime.datetime.strftime(get_date, '%Y-%m-%d')\n",
    "    print(get_date, end='|')\n",
    "    data = get(base_url+html_date)\n",
    "    if data == None:\n",
    "        print(\"none data\")\n",
    "        continue\n",
    "    # As I mentioned above this is an easy site, I can just use Pandas read_html to extract the tables efficiently\n",
    "    # add code to deal with pandas read exception\n",
    "    scrapeDict[get_date] = pd.read_html(data)\n",
    "    \n",
    "# Over here I have to manually incorporate the holidays so to delete them after downloading, for these dates the scraped data are incorrect\n",
    "# This procedure works after the pickle file is created and run for subsequent times.\n",
    "# For first time download, this module here will have error and exited - please continue to next module for the scraping.\n",
    "# opt_out_days: Opting out public holidays\n",
    "opt_out_days = ['2019-11-28', '2019-12-25', '2020-01-01', '2020-01-20', '2020-02-17', '2020-04-10',\n",
    "                '2020-05-25', '2020-07-03', '2020-09-07', \"2020-11-26\", \"2020-12-25\", '2021-01-01', '2021-01-18', '2021-02-15', '2021-04-02',\n",
    "                '2021-05-31', '2021-07-05', '2021-09-06', \"2021-11-25\", \"2021-12-24\", \"2022-01-17\"]\n",
    "# Getting rid of holidays listed under [opt_out_days]\n",
    "print(len(scrapeDict.keys()))\n",
    "for i in opt_out_days:\n",
    "    try: \n",
    "        del scrapeDict[datetime.datetime.strptime(i,'%Y-%m-%d').date()]\n",
    "    except KeyError:\n",
    "        continue\n",
    "print('Adjusted for opt-out-days:', len(scrapeDict.keys())) \n",
    "\n",
    "# save to a pickle\n",
    "outfile = open(pickle_file,\"wb\")\n",
    "pickle.dump(scrapeDict,outfile)\n",
    "outfile.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a740dbd4-3db3-441b-bcc5-54c8c721ec66",
   "metadata": {},
   "source": [
    "## Run the following cell to load data from the saved pickle file\n",
    "### This then formats the data similar to how totalpc.csv has it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821cdf1c-3fb7-48d8-a2df-81cd4a0da62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(pickle_file,\"rb\")\n",
    "df = pd.DataFrame(pickle.load(infile))\n",
    "infile.close()\n",
    "print(df.shape)\n",
    "print('Done!')\n",
    "# now remove un-needed data from the dataframe\n",
    "# format: DATE(mm/dd/yyyy) | CALLS | PUTS | TOTAL | P/C Ratio\n",
    "df = df.transpose()\n",
    "print(df.shape)\n",
    "df = df.drop(axis='columns',columns=range(2,9))\n",
    "print(df.shape)\n",
    "# turn name of series into another column\n",
    "# eventually to turn it into the index\n",
    "df['date']=df.apply(lambda row : row.name, axis = 1)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.set_index('date')\n",
    "df['calls']=df.apply(lambda row : row.iloc[1].iloc[1][1], axis=1)\n",
    "df['puts']=df.apply(lambda row : row.iloc[1].iloc[1][2], axis=1)\n",
    "df['total']=df.apply(lambda row : row.iloc[1].iloc[1][3], axis=1)\n",
    "df['p_c_ratio']=df.apply(lambda row : row.iloc[0].iloc[0][1], axis=1)\n",
    "df = df.drop(axis='columns',columns=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53588f0-dc36-4b8b-8b96-694886843a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478d8615-e35f-4290-8f18-57b6e710bb1a",
   "metadata": {},
   "source": [
    "## The following cell loads totalpc.csv\n",
    "### It then appends the data and saves it as a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2192ba3-51d0-408e-882e-873c22a166cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.read_csv(\"totalpc_archive.csv\")\n",
    "total_df.columns=['date','calls','puts','total','p_c_ratio']\n",
    "total_df=total_df.drop(labels=[0,1], axis=0)\n",
    "total_df['date'] = pd.to_datetime(total_df['date'])\n",
    "total_df = total_df.set_index('date')\n",
    "total_df.head()\n",
    "# sanity checks\n",
    "print(df.shape)\n",
    "print(total_df.shape)\n",
    "final_df = pd.concat([total_df,df], axis=0, verify_integrity=True, sort=True)\n",
    "print(final_df.shape)\n",
    "#cleanup\n",
    "final_df=final_df.dropna(axis=0)\n",
    "print(final_df.shape)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54070180-5166-48a6-a60c-8444fc45c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"totalpc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b3451-6dd2-46fe-b961-2afe17f270df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
